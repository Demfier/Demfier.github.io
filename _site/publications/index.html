<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Gaurav  Sahu | publications</title>
<meta name="description" content="A small space in the cloud I can safely refer to as my own. It contains essential pieces of my life until now and my aspirations.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000/">
       <span class="font-weight-bold">Gaurav</span>   Sahu
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/art/">
                art
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">publications by categories in reversed chronological order.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EACL</abbr>
    
  
  </div>

  <div id="sahu-2021-adaptive" class="col-sm-8">
    
      <div class="title">
        
          <a href="https://arxiv.org/pdf/1911.03821.pdf" target="_blank">Adaptive Fusion Techniques for Multimodal Data (To Appear)</a>
        
      </div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Sahu, Gaurav</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="https://ov-research.uwaterloo.ca/" target="_blank">Vechtomova, Olga</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Effective fusion of data from multiple modalities, such as video, speech, and text, is challenging due to the heterogeneous nature of multimodal data. In this paper, we propose adaptive fusion techniques that aim to model context from different modalities effectively. Instead of defining a deterministic fusion operation, such as concatenation, for the network, we let the network decide “how” to combine a given set of multimodal features more effectively. We propose two networks: 1) Auto-Fusion, which learns to compress information from different modalities while preserving thecontext, and 2) GAN-Fusion, which regularizes the learned latent space given context from complementing modalities. A quantitative evaluation on the tasks of multimodal machine translation and emotion recognition suggests that our lightweight, adaptive networks can better model context from other modalities than existing methods, many of which employ massive transformer-based networks.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">COLING</abbr>
    
  
  </div>

  <div id="khan-etal-2020-adversarial" class="col-sm-8">
    
      <div class="title">
        
          <a href="https://www.aclweb.org/anthology/2020.coling-main.441" target="_blank">Adversarial Learning on the Latent Space for Diverse Dialog Generation</a>
        
      </div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Khan, Kashif,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Sahu, Gaurav</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Balasubramanian, Vikash,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://lili-mou.github.io/" target="_blank">Mou, Lili</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="https://ov-research.uwaterloo.ca/" target="_blank">Vechtomova, Olga</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 28th International Conference on Computational Linguistics</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Generating relevant responses in a dialog is challenging, and requires not only proper modeling of context in the conversation, but also being able to generate fluent sentences during inference. In this paper, we propose a two-step framework based on generative adversarial nets for generating conditioned responses. Our model first learns a meaningful representation of sentences by autoencoding, and then learns to map an input query to the response representation, which is in turn decoded as a response sentence. Both quantitative and qualitative evaluations show that our model generates more fluent, relevant, and diverse responses than existing state-of-the-art methods.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ISMIR</abbr>
    
  
  </div>

  <div id="vechtomova-etal-2020-generation" class="col-sm-8">
    
      <div class="title">
        
          <a href="https://www.aclweb.org/anthology/2020.nlp4musa-1.7" target="_blank">Generation of lyrics lines conditioned on music audio clips</a>
        
      </div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://ov-research.uwaterloo.ca/" target="_blank">Vechtomova, Olga</a>,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Sahu, Gaurav</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="https://ddhruvkr.github.io/" target="_blank">Kumar, Dhruv</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 1st Workshop on NLP for Music and Audio (NLP4MusA) at ISMIR</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a system for generating novel lyrics lines conditioned on music audio. A bimodal neural network model learns to generate lines conditioned on any given short audio clip. The model consists of a spectrogram variational autoencoder (VAE) and a text VAE. Both automatic and human evaluations demonstrate effectiveness of our model in generating lines that have an emotional impact matching a given audio clip. The system is intended to serve as a creativity tool for songwriters.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Thesis</abbr>
    
  
  </div>

  <div id="sahu2020adaptive" class="col-sm-8">
    
      <div class="title">
        
          <a href="http://hdl.handle.net/10012/16194" target="_blank">Adaptive Fusion Techniques for Effective Multimodal Deep Learning</a>
        
      </div>
      <div class="author">
        
          
          
          
          

          
            
              <em>Sahu, Gaurav</em>
            
          
        
      </div>

      <div class="periodical">
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Effective fusion of data from multiple modalities, such as video, speech, and text, is a challenging task due to the heterogeneous nature of multimodal data. In this work, we propose fusion techniques that aim to model context from different modalities effectively. Instead of defining a deterministic fusion operation, such as concatenation, for the network, we let the network decide “how” to combine given multimodal features more effectively. We propose two networks: 1) Auto-Fusion network, which aims to compress information from different modalities while preserving the context, and 2) GAN-Fusion, which regularizes the learned latent space given context from complementing modalities. A quantitative evaluation on the tasks of multimodal machine translation and emotion recognition suggests that our adaptive networks can better model context from other modalities than all existing methods, many of which employ massive transformer-based networks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="cohen2020digital" class="col-sm-8">
    
      <div class="title">
        
          <a href="https://www.scirp.org/journal/paperinforcitation.aspx?paperid=100127" target="_blank">Digital Literacy for Secondary School Students: Using Computer Technology to Educate about Credibility of Content Online</a>
        
      </div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://cs.uwaterloo.ca/~rcohen/" target="_blank">Cohen, Robin</a>,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Parmentier, Alexandre,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Melo, Glaucia,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Sahu, Gaurav</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Annamalai, Aswin,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Chi, Sheldon,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Clokie, Trevor,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Farrag, Amir,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Naik, Abdul,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Naseem, Syed,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and others, 
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Creative Education Journal special issue on Education and Information Technology</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper presents an approach to educate secondary school students in the province of Ontario about the credibility of online content. The critical focus here is on integrating computer technology into the teaching of the topic; how to introduce the material in classroom settings with respect to the current curriculum is also outlined. Contrast with an existing proposal for digital literacy developed by historians at Stanford University is provided at the outset. In all, the value of appealing to the current digital experiences of students, when revealing the potential for misinformation, is the critical message. Exploration of social media environments popular with youth and opportunities for game-based quizzes for interactive engagement are both advocated.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="sahu2019multimodal" class="col-sm-8">
    
      <div class="title">
        
          <a href="https://arxiv.org/abs/1904.06022" target="_blank">Multimodal speech emotion recognition and ambiguity resolution</a>
        
      </div>
      <div class="author">
        
          
          
          
          

          
            
              <em>Sahu, Gaurav</em>
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:1904.06022</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Identifying emotion from speech is a non-trivial task pertaining to the ambiguous definition of emotion itself. In this work, we adopt a feature-engineering based approach to tackle the task of speech emotion recognition. Formalizing our problem as a multi-class classification problem, we compare the performance of two categories of models. For both, we extract eight hand-crafted features from the audio signal. In the first approach, the extracted features are used to train six traditional machine learning classifiers, whereas the second approach is based on deep learning wherein a baseline feed-forward neural network and an LSTM-based classifier are trained over the same features. In order to resolve ambiguity in communication, we also include features from the text domain. We report accuracy, f-score, precision, and recall for the different experiment settings we evaluated our models in. Overall, we show that lighter machine learning based models trained over a few hand-crafted features are able to achieve performance comparable to the current deep learning based state-of-the-art method for emotion recognition.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="krishna-etal-2018-free" class="col-sm-8">
    
      <div class="title">
        
          <a href="https://www.aclweb.org/anthology/D18-1276" target="_blank">Free as in Free Word Order: An Energy Based Model for Word Segmentation and Morphological Tagging in Sanskrit</a>
        
      </div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Krishna, Amrith,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Santra, Bishal,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Bandaru, Sasi Prasanth,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Sahu, Gaurav</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Sharma, Vishnu Dutt,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Satuluri, Pavankumar,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="https://cse.iitkgp.ac.in/~pawang/" target="_blank">Goyal, Pawan</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The configurational information in sentences of a free word order language such as Sanskrit is of limited use. Thus, the context of the entire sentence will be desirable even for basic processing tasks such as word segmentation. We propose a structured prediction framework that jointly solves the word segmentation and morphological tagging tasks in Sanskrit. We build an energy based model where we adopt approaches generally employed in graph based parsing techniques (McDonald et al., 2005a; Carreras, 2007). Our model outperforms the state of the art with an F-Score of 96.92 (percentage improvement of 7.06%) while using less than one tenth of the task-specific training data. We find that the use of a graph based approach instead of a traditional lattice-based sequential labelling approach leads to a percentage gain of 12.6% in F-Score for the segmentation task.</p>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    <!-- &copy; Copyright 2021 Gaurav  Sahu. -->
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    
    Last updated: January 27, 2021.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
